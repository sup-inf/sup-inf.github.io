<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LOVINEE</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://rm-rf.moe/"/>
  <updated>2019-02-02T04:24:03.096Z</updated>
  <id>https://rm-rf.moe/</id>
  
  <author>
    <name>hsm</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>adaptive-robust-backstepping</title>
    <link href="https://rm-rf.moe/2019/02/02/adaptive-robust-backstepping/"/>
    <id>https://rm-rf.moe/2019/02/02/adaptive-robust-backstepping/</id>
    <published>2019-02-02T04:24:03.000Z</published>
    <updated>2019-02-02T04:24:03.096Z</updated>
    
    <summary type="html">
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Backstepping Technique</title>
    <link href="https://rm-rf.moe/2019/02/02/backstepping/"/>
    <id>https://rm-rf.moe/2019/02/02/backstepping/</id>
    <published>2019-02-02T02:20:38.000Z</published>
    <updated>2019-02-02T04:29:34.489Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Single-integrator-Backstepping"><a href="#Single-integrator-Backstepping" class="headerlink" title="Single-integrator Backstepping"></a>Single-integrator Backstepping</h1><p>Consider the system  </p><script type="math/tex; mode=display">\left\{\begin{aligned}&\dot{x} = f(x) + g(x)\eta,\ g(x) > 0\\&\dot{\eta} = u\end{aligned}\right.</script><p>where, <script type="math/tex">x,y \in \mathbb{R}</script> are the states, <script type="math/tex">u \in \mathbb{R}</script> is the control input, and <script type="math/tex">f,g</script> are assumed to be known.<br><a id="more"></a></p><blockquote><p><em>Objective:</em> <script type="math/tex">x \to 0</script> as <script type="math/tex">t \to \infty</script> </p></blockquote><div align="center">  <img width="350" src="/2019/02/02/backstepping/1.png"></div><h2 id="LaShalle-Yoshizawa-Theorem"><a href="#LaShalle-Yoshizawa-Theorem" class="headerlink" title="LaShalle-Yoshizawa Theorem"></a>LaShalle-Yoshizawa Theorem</h2><p>Let <script type="math/tex">x = 0</script> be an equilibrium point of <script type="math/tex">\dot{x} = f(x,t)</script> and suppose <script type="math/tex">f</script> is locally Lipschitz in <script type="math/tex">x</script> uniformly in <script type="math/tex">t</script>. Let <script type="math/tex">V: \mathbb{R}^n \to \mathbb{R}_+</script> be a continuously differentiable, positive define and radially unbounded function <script type="math/tex">V(x)</script> such that:    </p><script type="math/tex; mode=display">\dot{V} = \frac{\partial V}{\partial x}(x)f(x,t)\leq -W(x) \leq 0,\ \ \ \ \ \  \forall t \geq 0, \  \ \forall x \in \mathbb{R}</script><p>Where <script type="math/tex">W</script> is a continuous function. Then all solutions of <script type="math/tex">\dot{x} = f(x,t)</script>                                                      are globally uniformly bounded and satisfy</p><script type="math/tex; mode=display">\lim_{t \to \infty} W(x(t)) = 0</script><p>If <script type="math/tex">W(x)</script> is positive define, then equilibrium            is <strong>Globally Uniformly Asymptotically Stable</strong>(GUAS)</p><h2 id="Method"><a href="#Method" class="headerlink" title="Method"></a>Method</h2><div align="center">  <img width="400" src="/2019/02/02/backstepping/2.png"></div><blockquote><p><em>Objective1:</em> <script type="math/tex">x \to 0</script><br><em>Objective2:</em> <script type="math/tex">\eta \to \eta_d</script>   </p></blockquote><p>Subsystem <script type="math/tex">\dot{x} = F(x)</script>, where <script type="math/tex">F(x) \triangleq f(x)+g(x)\eta_d</script><br>Choose Lyapunov Function <script type="math/tex">V_x</script></p><script type="math/tex; mode=display">\dot{V}_x = \frac{\partial V_x}{\partial x}(f(x)+g(x)\eta_d)\leq -W(x)</script><p>add and minus <script type="math/tex">g(x)\eta_d</script></p><script type="math/tex; mode=display">\left\{\begin{aligned}&\dot{x} = f(x) + g(x)\eta\\&\dot{\eta} = u\end{aligned}\right. \Rightarrow\left\{\begin{aligned}&\dot{x} = f(x) + g(x)\eta + g(x)\eta_d- g(x)\eta_d\\&\dot{\eta} = u\end{aligned}\right.</script><p>Regroup</p><script type="math/tex; mode=display">\left\{\begin{aligned}&\dot{x} =\underbrace{f(x) + g(x)\eta_d}_{F(x)} + g(x)(\underbrace{\eta -\eta_d}_{error\ e_1})\\&\dot{\eta} = u\end{aligned}\right.\Rightarrow \left\{\begin{aligned}&\dot{x} =f(x) + g(x)\eta_d + g(x)e_1\\&\dot{e_1} = u - \dot \eta_d \triangleq v_1\end{aligned}\right.</script><blockquote><p><em>Objective:</em> Find Control <script type="math/tex">v_1</script> stabilizing the system at <script type="math/tex">e_1 = 0</script> </p></blockquote><p>Lyapunov Candidate:</p><script type="math/tex; mode=display">V_1(x, e_1) = V_x(x) + \frac{1}{2}e_1^2</script><script type="math/tex; mode=display">\begin{aligned}\dot{V}_1 &= \frac{\partial V_x}{\partial x} \dot x + e_1\dot{e}_1 \\&= \frac{\partial V_x}{\partial x}(f(x) + g(x)\eta_d + g(x)e_1)+e_1 v_1\\&= \underbrace{\frac{\partial V_x}{\partial x}(f(x) + g(x)\eta_d)}_{\leq -W(x)} + \underbrace{e_1(\frac{\partial V_x}{\partial x}g(x) + v_1)}_{shall\  \leq 0}\end{aligned}</script><p>Let <script type="math/tex">v_1 = -\frac{\partial V_x}{\partial x}g(x) - k_1 e_1</script>, with <script type="math/tex">k_1 > 0</script></p><script type="math/tex; mode=display">\Rightarrow \dot V_1 = -W(x)-k_1e_1^2 \leq -W(x) < 0</script><p>Actual Control Law is</p><script type="math/tex; mode=display">u = v_1 + \dot \eta_d = -\frac{\partial V_x}{\partial x}g(x) + k_1 (\eta-\eta_d)+\frac{\partial \eta_d}{\partial x}\dot{x}</script><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><ul><li>The designer can start the design process at the known-stable system and “back out” new controllers that progressively stabilize each outer subsystem. </li><li>Any strict-feedback system can be feedback stabilized using a straightforward procedure.</li><li>Backstepping design doesn’t require a differentiator.</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Single-integrator-Backstepping&quot;&gt;&lt;a href=&quot;#Single-integrator-Backstepping&quot; class=&quot;headerlink&quot; title=&quot;Single-integrator Backstepping&quot;&gt;&lt;/a&gt;Single-integrator Backstepping&lt;/h1&gt;&lt;p&gt;Consider the system  &lt;/p&gt;
&lt;script type=&quot;math/tex; mode=display&quot;&gt;
\left\{
\begin{aligned}
&amp;\dot{x} = f(x) + g(x)\eta,\ g(x) &gt; 0\\
&amp;\dot{\eta} = u
\end{aligned}
\right.&lt;/script&gt;&lt;p&gt;where, &lt;script type=&quot;math/tex&quot;&gt;x,y \in \mathbb{R}&lt;/script&gt; are the states, &lt;script type=&quot;math/tex&quot;&gt;u \in \mathbb{R}&lt;/script&gt; is the control input, and &lt;script type=&quot;math/tex&quot;&gt;f,g&lt;/script&gt; are assumed to be known.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Neural Network-Assignment</title>
    <link href="https://rm-rf.moe/2019/02/01/neural-network/"/>
    <id>https://rm-rf.moe/2019/02/01/neural-network/</id>
    <published>2019-02-01T09:31:52.000Z</published>
    <updated>2019-02-01T10:25:35.223Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Artificial neural networks (ANNs) are computational models that are loosely inspired by their biological counterparts. Artificial neurons, which are elementary units in an ANN, appears to have been first introduced by McCulloch and Pitts in 1943 to demonstrate the biological neurons using algorithms and mathematics. An ANN is based on a set of connected artificial neurons. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. Since the early work of Hebb, who proposed cell assembly theory which attempted to explain synaptic plasticity, considerable effort has been devoted to ANNs. Though perceptron was created by Rosenblatt for pattern recognition by training an adaptive  McCulloch-Pitts neuron model, it is incapable of processing the exclusive-or circuit. A key trigger for renewed interest in neural networks and learning was Werbos’s back-propagation algorithm, which makes the training of multi-layer networks feasible and efficient.<br><a id="more"></a></p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>Multi-Layered Perceptron(MLP) gains prestige as a feedforward neural network, which has been the most widely used neural networks. Error Back Propagation(BP) algorithm is implemented as a training method for MLPs.</p><h2 id="Multi-Layered-Perceptron"><a href="#Multi-Layered-Perceptron" class="headerlink" title="Multi-Layered Perceptron"></a>Multi-Layered Perceptron</h2><p>MLP, which is a fully-connected feedforward neural network, uses a supervised learning methods. An MLP consists of an input layer, an output layer and at least one hidden layer. The fundamental structure of MLP is shown schematically in Fig. 1:</p><div align="center">  <img width="300" src="/2019/02/01/neural-network/1.png"></div><p>Considering the input layer, neurons take effect on passing the information, i.e. the activation functions of this layer are identity functions. Each neurons, who consist the input layer, completely connect with the first hidden layer, thus those neurons hold multiple outputs. By giving <script type="math/tex">M</script> neurons in input layer and <script type="math/tex">N</script> neurons in hidden layer, each connection is assigned a weight <script type="math/tex">v_{nm}</script>. A bias term added to total weighted sum of inputs to serve as threshold to shift the activation function.The propagation function computes the hidden layer input <script type="math/tex">X_j</script> to the neuron <script type="math/tex">x(j)</script> from the outputs of predecessor neurons <script type="math/tex">Z_j</script> has the form</p><script type="math/tex; mode=display">X_j = \sum_i z_j v_{ij} - \theta</script><p>where <script type="math/tex">\theta</script> donates the bias term.</p><p>The activation function, which defines the output of the node, were chosen as “Hyperbolic tangent” and “Softsign” in this case. Equations, plot and derivative are given by TABLE 1.</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/2.png"></div><p>MLPs’ scheme essentially has three features:</p><ul><li>There are no connections between the neurons which are in the layers, and neurons shall not have connections with themselves.</li><li>Full connections exist only between the nearby layers.</li><li>It consists of two aspects: the feed-forward transmission of information and the feed-back transmission of error.</li></ul><h2 id="Error-Back-Propagation"><a href="#Error-Back-Propagation" class="headerlink" title="Error Back Propagation"></a>Error Back Propagation</h2><p>Back Propagation is a method used in ANNs to calculate a gradient that is needed in the calculation of the weights between layers, and it is commonly used by the gradient descent optimization algorithm. Calculating adjustments of weights in Fig. 1 is chosen as an example to introduce BP algorithm. We represent the parameter <script type="math/tex">\delta</script> with respect to <script type="math/tex">i</script>-th layer by the notation <script type="math/tex">\delta^{(i)}</script>. </p><p>For calculating the weights between hidden layer and output layer, loss function is defined as:</p><script type="math/tex; mode=display">E_q = \frac{1}{2} \sum^{K}_{j=1} (\widehat{y}_{qj}-y_{qj})^2</script><p>where <script type="math/tex">\widehat{y}_{qj}</script> is expected output, <script type="math/tex">y_{qj}</script> is actual output. Based on gradient descent algorithm with learning rate <script type="math/tex">\mu</script>, which controls how much algorithm adjusting the weights of neuron network with respect the loss gradient, and the input of activation function <script type="math/tex">s_j</script>, we have</p><script type="math/tex; mode=display">\Delta w_{ji} = -\mu \frac{\partial E_q}{\partial w_{ji}} = -\mu \frac{\partial E_q}{\partial s^{(3)}_j}\frac{\partial s^{(3)}_j}{\partial w_{ji}}</script><p>with</p><script type="math/tex; mode=display">\frac{\partial s^{(3)}_j}{\partial w_{ji}} = \frac{\partial}{\partial w_{ji}}(\sum^N_{h=1} w_{jh}x_h) = x_i</script><p>and we have</p><script type="math/tex; mode=display">\frac{\partial E_q}{\partial s^{(3)}_j} = -[\widehat{y}_{qj}-f(s^{(3)}_j)]f'(s^{(3)}_j) \triangleq \delta^{(3)}_j</script><p>where <script type="math/tex">\delta_j</script> donates the error of the output layer, thus, the adjustment of $w$ shall be:</p><script type="math/tex; mode=display">\Delta w_{ji} = \mu \delta_j x_i</script><p>Consider calculating the weights between input layer and output layer:</p><script type="math/tex; mode=display">\Delta v_{ji} = -\mu \frac{\partial E_q}{\partial v_{ji}} = -\mu \frac{\partial E_q}{\partial s^{(2)}_j}\frac{\partial s^{(2)}_j}{\partial v_{ji}}</script><p>with</p><script type="math/tex; mode=display">\frac{\partial s^{(2)}_j}{\partial v_{ji}} = \frac{\partial}{\partial v_{ji}}(\sum^M_{h=1} v_{jh}z_h) = z_i</script><p>and</p><script type="math/tex; mode=display">\frac{\partial E_q}{\partial s^{(2)}_j} = \frac{\partial E_q}{\partial x_j}\frac{\partial x_j}{\partial s^{(2)}_j}</script><p>where</p><script type="math/tex; mode=display">\frac{\partial E_q}{\partial x_j}  = -\sum^K_{k = 1}(\widehat{y}_{qk} - y_k)f'(s_k^{(3)})w_{kj}\triangleq -\delta_j^{(2)}</script><p>thus</p><script type="math/tex; mode=display">v_{ji} = -\mu \frac{\partial E_q}{\partial v_{ji}} = \mu \delta^{(2)}_j z_j</script><p>In summary,  the equations describing the adjustment of weights is:</p><script type="math/tex; mode=display">w^{(l)}_{ji}[k+1] = w^{(l)}_{ji}[k] + \mu^{(l)}\delta_j^{(l)}x_i^{(l-1)}</script><p>where <script type="math/tex">\delta^{(l)}_j</script> for output layer is:</p><script type="math/tex; mode=display">\delta^{(l)}_j = (\widehat{y} - y^{(l)})f'(s^{(l)}_j)</script><p>for hidden layer and input layer is</p><script type="math/tex; mode=display">\delta^{(l)}_j = f'(s^{(l)}_j)\sum_{k = 1}^{n_{l+1}}\delta^{(l+1)}_k w^{(l+1)}_{kj}</script><p>respectively.</p><p>The process of training MLP with BP algorithm is shown in Algorithm 1.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;algorithm&#125;</span><br><span class="line">\caption&#123;Error Back Propagation Algorithm&#125;</span><br><span class="line">\begin&#123;algorithmic&#125;[1]</span><br><span class="line">\STATE SET initial weights $w^&#123;l&#125;_&#123;ji&#125;$, bias $\theta^&#123;l&#125;_&#123;j&#125;$;</span><br><span class="line">\STATE SET training sample $(x_q, y_q)$;</span><br><span class="line">\FOR&#123;each $iter \in [1, maxEpoch]$&#125;</span><br><span class="line">\STATE $x^&#123;(l)&#125;  = f(s^&#123;(l)&#125;) = f(W^&#123;(l)&#125;x^&#123;(l-1)&#125;)$</span><br><span class="line">\STATE $\delta^&#123;(l)&#125;_j = (\widehat&#123;y&#125; - y^&#123;(l)&#125;)f&apos;(s^&#123;(l)&#125;_j)$ for output layer</span><br><span class="line">\STATE $\delta^&#123;(l)&#125;_j = f&apos;(s^&#123;(l)&#125;_j)\sum_&#123;k = 1&#125;^&#123;n_&#123;l+1&#125;&#125;\delta^&#123;(l+1)&#125;_k w^&#123;(l+1)&#125;_&#123;kj&#125;$ for other layers</span><br><span class="line">\STATE $w^&#123;(l)&#125;_&#123;ji&#125;[k + 1] = w^&#123;(l)&#125;_&#123;ji&#125;[k] + \mu \delta_j^&#123;(l)&#125;x_i^&#123;(l-1)&#125;$</span><br><span class="line">\STATE $\theta^&#123;(l)&#125;_j[k + 1] = \theta^&#123;(l)&#125;_j[k] + \mu \delta_j^&#123;(l)&#125;$ </span><br><span class="line">\STATE Calculate $sumLoss = sumLoss + E_q$</span><br><span class="line">\IF&#123;$sumLoss/iter &lt; setLossTolerant$&#125;</span><br><span class="line">\STATE \textbf&#123;break&#125;;</span><br><span class="line">\ENDIF</span><br><span class="line">\ENDFOR</span><br><span class="line">\label&#123;code:ag1&#125;</span><br><span class="line">\end&#123;algorithmic&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>In this section, we have designed different model structures for addressing curve fitting problems.</p><h2 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h2><p>We designed two different structures, which are shown in Fig. 2.</p><div align="center">  <img width="300" src="/2019/02/01/neural-network/3.png"></div><div align="center">  <img width="350" src="/2019/02/01/neural-network/4.png"></div><p>The structure which has one hidden layer with 600 nodes was applied to fitting <script type="math/tex">f(x) = sin(x)</script> and <script type="math/tex">f(x_1, x_2) = \frac{sin(x_1)}{x_1} \frac{sin(x_2)}{x_2}</script>. For <script type="math/tex">y = |sin(x)|</script>, we chosen the structure with 2 hidden layers to reach a better regression performance. </p><h2 id="f-x-sin-x"><a href="#f-x-sin-x" class="headerlink" title="f(x) = sin(x)"></a><script type="math/tex">f(x) = sin(x)</script></h2><p>We trained the layers with 9 sample, using <script type="math/tex">E = \frac{1}{2} (\widehat{y} - y)^2</script> as the loss function. The training process will stop when <script type="math/tex">E_{avg} = (\sum_{i = 1}^{epoch} E_i) /epoch < 2\times 10^{-4}</script>. The results with 361 test samples are shown in Fig. 3:</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/5.png"></div><p>Runtime loss $E$ and average loss <script type="math/tex">E_{avg}</script> are shown in Fig. 4, where the testing value evaluated by the average loss function is <script type="math/tex">9.3599\times 10^{-5}</script>.</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/6.png"></div><p>We notice that the loss suffers a drastic drop within <script type="math/tex">50,000</script> epochs, then, to decrease the average loss <script type="math/tex">E_{avg}</script> from <script type="math/tex">0.1</script> to <script type="math/tex">0.002</script>, more than <script type="math/tex">1\times 10^6</script> epochs are needed. </p><p>The absolute error is shown in Fig. 5:</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/7.png"></div><p>Apparently, max error between actual value and predict value is less than 0.05, which shows the prestige performance when fitting <script type="math/tex">f(x) = sin(x)</script>.</p><h2 id="f-x-sin-x-1"><a href="#f-x-sin-x-1" class="headerlink" title="f(x) = |sin(x)|"></a><script type="math/tex">f(x) = |sin(x)|</script></h2><p>When it comes to a more complex nonlinear curve, the fitting error using elementary MLP structure which only has one hidden layer is far from satisfactory. A hidden layer with softsign activation function was added to the basic MLP structure. By implementing the structure with 9 training sample, we arrive at the following results:</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/8.png"></div><p>Fig. 7 shows the loss decreased over training epochs:</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/9.png"></div><p>From the figure above, we can conclude that though the loss suffers the chattering within 500,000 epochs, while the average loss obtains a smooth constant rate of descent. Though the training loss which is less than <script type="math/tex">1\times 10^{-5}</script> after <script type="math/tex">3\times 10^6</script> epochs yields the guaranteed performance on training samples, as we can observed in Fig. 6, the test error(more than 0.2) is unbearable for some test samples. We assumed lacking of train samples has brought about the overfitting of neural network, thus we expanded the training sample with 29 independent (x, y) to reach the better performance. </p><p>When average loss is less than 0.005, we obtained the following result, as shown in Fig. 8:</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/10.png"></div><p>Apparently, <script type="math/tex">y = |sin(x)|</script> has a non-differentiable point in <script type="math/tex">(0, 2\pi)</script>. From Fig. 9, the point which is non-differentiable has the maximal error compared to other points. i.e. providing more training sample can not reinforce the ability of non-linear fitting for a certain MLP model. </p><div align="center">  <img width="350" src="/2019/02/01/neural-network/11.png"></div><p>Still, adding training sample improves the performance of prediction on test sets. From Fig. 9, though we have a test point which posses the error more than 0.1, absolute errors of other test points are less than 0.02. i.e. this approach provides more robust solutions than 9 training sample with the same loss tolerant. The training loss of this approach is shown in Fig. 10</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/12.png"></div><h2 id="f-x-y-frac-sin-x-x-frac-sin-y-y"><a href="#f-x-y-frac-sin-x-x-frac-sin-y-y" class="headerlink" title="f(x, y) = \frac{sin(x)}{x}\frac{sin(y)}{y}"></a><script type="math/tex">f(x, y) = \frac{sin(x)}{x}\frac{sin(y)}{y}</script></h2><p>In this section, let us consider a function which has two inputs. <script type="math/tex">11\times 11</script> samples are given for training with the basic MLP structure. The hidden layer has 500 nodes, and the training will stop when <script type="math/tex">E_{avg} < 0.002</script>. Fig. 11 shows the actual curve and the curve predicted with <script type="math/tex">21\times 21</script> samples respectively.</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/13.png"></div><div align="center">  <img width="350" src="/2019/02/01/neural-network/14.png"></div><p>We plot <script type="math/tex">E</script> and <script type="math/tex">E_{avg}</script>, varying <script type="math/tex">epoch</script> from <script type="math/tex">[1, 100,000]</script> in Fig. 12, from which we observe <script type="math/tex">E_{avg}</script> decrease rapidly from 1.1 to 0.1 within 1000 epochs. Thus, the training process shows the efficiency and robust performance of back propagation algorithm.</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/15.png"></div><p>The test error is shown as Fig. 13, from which we observe the maximal training error is less than 0.15.</p><div align="center">  <img width="350" src="/2019/02/01/neural-network/16.png"></div><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>In this report, we implemented multilayer perceptron to provide the solutions to three specific non-linear functions. Simulation results illustrates the accuracy of artificial neural networks and the efficiency of back propagation algorithm. </p><h1 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h1><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% For y = sin(x)</span></span><br><span class="line">clear</span><br><span class="line">sample_num = <span class="number">9</span>; N1 = <span class="number">600</span>; Lr = <span class="number">0.005</span>;</span><br><span class="line">train_x = <span class="built_in">zeros</span>(<span class="number">1</span>,sample_num);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:sample_num</span><br><span class="line">    train_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(sample_num<span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">train_y = <span class="built_in">sin</span>(train_x);</span><br><span class="line"></span><br><span class="line">w1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">w2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">sum_e = <span class="number">0</span>; er = []; <span class="built_in">i</span> = <span class="number">0</span>; ei = [];</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">j</span> = unidrnd(sample_num);</span><br><span class="line">    in = train_x(<span class="built_in">j</span>);</span><br><span class="line">    L1 = w1*in+b1;</span><br><span class="line">    L2 = <span class="built_in">tanh</span>(L1);</span><br><span class="line">    L3 = sum(w2.*L2)+b2;</span><br><span class="line">    e = train_y(<span class="built_in">j</span>)-L3;</span><br><span class="line">    sum_e = sum_e + <span class="number">0.5</span>*e^<span class="number">2</span>;</span><br><span class="line">    er = [er sum_e/<span class="built_in">i</span>];</span><br><span class="line">    ei = [ei e];</span><br><span class="line">    sum_e/<span class="built_in">i</span></span><br><span class="line">    <span class="keyword">if</span> sum_e/<span class="built_in">i</span> &lt; <span class="number">0.002</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    delta_2 = e;</span><br><span class="line">    delta_1 = (<span class="number">1</span>-<span class="built_in">tanh</span>(L1).^<span class="number">2</span>).*(delta_2*w2);</span><br><span class="line">    w2 = w2+Lr*delta_2.*L2;</span><br><span class="line">    w1 = w1+Lr*delta_1*in;</span><br><span class="line">    b1 = b1+Lr*delta_1;</span><br><span class="line">    b2 = b2+Lr*e;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">test_x = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">1000</span>);</span><br><span class="line">test_y = [];</span><br><span class="line"><span class="built_in">true</span>_y = [];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">1000</span></span><br><span class="line">    test_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(<span class="number">1000</span><span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = test_x</span><br><span class="line">    test_y = [test_y sum(w2.*<span class="built_in">tanh</span>(w1*<span class="built_in">i</span>+b1))+b2];</span><br><span class="line">    <span class="built_in">true</span>_y = [<span class="built_in">true</span>_y <span class="built_in">sin</span>(<span class="built_in">i</span>)];</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">plot</span>(test_x, test_y, test_x, <span class="built_in">true</span>_y);</span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="comment">%% For y = abs(sin(x))</span></span><br><span class="line">clear</span><br><span class="line">sample_num = <span class="number">29</span>; N1 = <span class="number">500</span>; Lr = <span class="number">0.002</span>; </span><br><span class="line">train_x = <span class="built_in">zeros</span>(<span class="number">1</span>,sample_num);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:sample_num</span><br><span class="line">    train_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(sample_num<span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">train_y = <span class="built_in">abs</span>(<span class="built_in">sin</span>(train_x));</span><br><span class="line"></span><br><span class="line">w1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b3 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">w2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(N1,N1);</span><br><span class="line">w3 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">L3 = <span class="built_in">zeros</span>(<span class="number">1</span>,N1);</span><br><span class="line">delta_1 = <span class="built_in">zeros</span>(<span class="number">1</span>,N1);</span><br><span class="line">sum_e = <span class="number">0</span>; er = []; ei = []; <span class="built_in">i</span> = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    s = unidrnd(sample_num);</span><br><span class="line">    in = train_x(s);</span><br><span class="line">    L1 = w1*in+b1;</span><br><span class="line">    L2 = softsign(L1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:N1 </span><br><span class="line">        L3(<span class="built_in">j</span>) = sum(w2(<span class="built_in">j</span>,:).*L2)+b2(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    L4 = <span class="built_in">tanh</span>(L3);</span><br><span class="line">    L5 = sum(w3.*L4)+b3;</span><br><span class="line">    e = train_y(s)-L5;</span><br><span class="line">    sum_e = sum_e + <span class="number">0.5</span>*e^<span class="number">2</span>;</span><br><span class="line">    sum_e/<span class="built_in">i</span></span><br><span class="line">    <span class="keyword">if</span> sum_e/<span class="built_in">i</span> &lt; <span class="number">0.005</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    er = [er sum_e/<span class="built_in">i</span>];</span><br><span class="line">    ei = [ei e];</span><br><span class="line">    delta_3 = e;</span><br><span class="line">    delta_2 = (<span class="number">1</span>-<span class="built_in">tanh</span>(L2).^<span class="number">2</span>).*(delta_3*w3);</span><br><span class="line">    temp = dsoftsign(L1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:N1</span><br><span class="line">        delta_1(<span class="built_in">j</span>) = temp(<span class="built_in">j</span>).*...</span><br><span class="line">        sum(delta_2.*w2(<span class="built_in">j</span>,:));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    w3 = w3+Lr*delta_3.*L4;</span><br><span class="line">    w2 = w2+Lr*delta_2.*L2;</span><br><span class="line">    w1 = w1+Lr*delta_1*in;</span><br><span class="line">    b1 = b1+Lr*delta_1;</span><br><span class="line">    b2 = b2+Lr*delta_2;</span><br><span class="line">    b3 = b3+Lr*delta_3;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">test_x = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">1000</span>); t = <span class="built_in">zeros</span>(<span class="number">1</span>,N1);</span><br><span class="line">test_y = []; <span class="built_in">true</span>_y = [];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">1000</span></span><br><span class="line">    test_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(<span class="number">1000</span><span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = test_x</span><br><span class="line">    var = softsign(w1*<span class="built_in">i</span>+b1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:N1</span><br><span class="line">        t(<span class="built_in">j</span>) = sum(w2(<span class="built_in">j</span>,:).*var)+b2(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    test_y = [test_y sum(w3.*<span class="built_in">tanh</span>(t))+b3];</span><br><span class="line">    <span class="built_in">true</span>_y = [<span class="built_in">true</span>_y <span class="built_in">abs</span>(<span class="built_in">sin</span>(<span class="built_in">i</span>))];</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">plot</span>(test_x, test_y, test_x, <span class="built_in">true</span>_y);</span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="comment">%% For z = sin(x)/x*sin(y)/y</span></span><br><span class="line">clear</span><br><span class="line">sample_num = <span class="number">20</span>; N1 = <span class="number">500</span>; Lr = <span class="number">0.006</span>;</span><br><span class="line">x1 = <span class="built_in">linspace</span>(<span class="number">-10</span>,<span class="number">10</span>,sample_num);</span><br><span class="line">x2 = <span class="built_in">linspace</span>(<span class="number">-10</span>,<span class="number">10</span>,sample_num);</span><br><span class="line">y = <span class="built_in">zeros</span>(sample_num,sample_num);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:sample_num</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:sample_num</span><br><span class="line">        y(<span class="built_in">i</span>,<span class="built_in">j</span>) = value(x1(<span class="built_in">i</span>),x2(<span class="built_in">j</span>));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">w1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">2</span>,N1);</span><br><span class="line">b1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">w2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">er = []; sum_e = <span class="number">0</span>; <span class="built_in">i</span> = <span class="number">0</span>; ei = [];</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">i</span> =  <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    batch1 = unidrnd(sample_num);</span><br><span class="line">    batch2 = unidrnd(sample_num);</span><br><span class="line">    in1 = x1(batch1);</span><br><span class="line">    in2 = x2(batch2);</span><br><span class="line">    L1 = w1(<span class="number">1</span>,:)*in1+w1(<span class="number">2</span>,:)*in2+b1;</span><br><span class="line">    L2 = <span class="built_in">tanh</span>(L1);</span><br><span class="line">    L3 = sum(w2.*L2)+b2;</span><br><span class="line">    e = y(batch1, batch2)-L3;</span><br><span class="line">    sum_e = sum_e + <span class="number">0.5</span>*e^<span class="number">2</span>;</span><br><span class="line">    sum_e/<span class="built_in">i</span></span><br><span class="line">    er = [er sum_e/<span class="built_in">i</span>];</span><br><span class="line">    ei = [ei e];</span><br><span class="line">    <span class="keyword">if</span> sum_e/<span class="built_in">i</span> &lt; <span class="number">0.002</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    delta_2 = e;</span><br><span class="line">    delta_1 = (<span class="number">1</span>-<span class="built_in">tanh</span>(L1).^<span class="number">2</span>).*(delta_2*w2);</span><br><span class="line">    w2 = w2+Lr*delta_2.*L2;</span><br><span class="line">    w1(<span class="number">1</span>,:) = w1(<span class="number">1</span>,:)+Lr*delta_1*in1;</span><br><span class="line">    w1(<span class="number">2</span>,:) = w1(<span class="number">2</span>,:)+Lr*delta_1*in2;</span><br><span class="line">    b1 = b1+Lr*delta_1;</span><br><span class="line">    b2 = b2+Lr*e;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">[X,Y] = <span class="built_in">meshgrid</span>(<span class="number">-10</span>:<span class="number">0.3</span>:<span class="number">10</span>);</span><br><span class="line">Z_ = (<span class="built_in">sin</span>(X)./X).*(<span class="built_in">sin</span>(Y)./Y);</span><br><span class="line">Z = <span class="built_in">zeros</span>(<span class="built_in">length</span>(X));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(X)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(X)</span><br><span class="line">        temp = <span class="built_in">tanh</span>(w1(<span class="number">1</span>, :)*X(<span class="built_in">i</span>, <span class="built_in">j</span>)...</span><br><span class="line">        +w1(<span class="number">2</span>,:)*Y(<span class="built_in">i</span>, <span class="built_in">j</span>)+b1);</span><br><span class="line">        Z(<span class="built_in">i</span>, <span class="built_in">j</span>) = sum(w2.*temp)+b2;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">mesh(X,Y,Z)</span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">mesh(X,Y,Z_)</span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">softsign</span><span class="params">(x)</span></span></span><br><span class="line">y = x./(<span class="number">1</span>+<span class="built_in">abs</span>(x));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">dsoftsign</span><span class="params">(x)</span></span></span><br><span class="line">y = (<span class="number">1.</span>/(<span class="number">1</span>+x).^<span class="number">2</span>).*(x&gt;=<span class="number">0</span>)+(<span class="number">1.</span>/(<span class="number">1</span>-x).^<span class="number">2</span>).*(x&lt;<span class="number">0</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Artificial neural networks (ANNs) are computational models that are loosely inspired by their biological counterparts. Artificial neurons, which are elementary units in an ANN, appears to have been first introduced by McCulloch and Pitts in 1943 to demonstrate the biological neurons using algorithms and mathematics. An ANN is based on a set of connected artificial neurons. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. Since the early work of Hebb, who proposed cell assembly theory which attempted to explain synaptic plasticity, considerable effort has been devoted to ANNs. Though perceptron was created by Rosenblatt for pattern recognition by training an adaptive  McCulloch-Pitts neuron model, it is incapable of processing the exclusive-or circuit. A key trigger for renewed interest in neural networks and learning was Werbos’s back-propagation algorithm, which makes the training of multi-layer networks feasible and efficient.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithm" scheme="https://rm-rf.moe/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>Genetic Algorithm-Assignment</title>
    <link href="https://rm-rf.moe/2019/01/24/genetic-algorithm/"/>
    <id>https://rm-rf.moe/2019/01/24/genetic-algorithm/</id>
    <published>2019-01-24T07:09:42.000Z</published>
    <updated>2019-02-01T10:25:03.195Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Inspired by the process of natural selection and principle of genetics, genetic algorithms are implemented to handle various optimization problems. Basic theorem and  mathematical derivation has given by J. H. Holland. This research report is intended as a solution, performing optimization on two fitness functions. The reminder of this report is structured as follows. Section 2 reviews the relevant fundamentals of genetic algorithm, which includes the detailed structure with its component parts. In Section 3, the maximal of two fitness function are given with the optimal populations which contain exhaustive revolutionary process. Conclusions of the report are presented in Section 4.<br><a id="more"></a></p><h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>Genetic algorithm is constructed from distinct components, which are the chromosome encoding, the fitness function, selection, crossover and mutation. Starting with a randomly generated population of chromosomes, a genetic algorithm carried out a process of fitness based selection and recombination to produce a successor population, the next generation. During the recombination, parent chromosomes which encoded by binary strings are selected, depending on their fitness. In this case, those chromosomes who can obtain larger function values own larger fitness, and the possibilities of producing offspring increase. Although crossover may decrease the fitness of some individuals, the population fitness will be improved. Mutations ensure genetic diversity within the genetic pool of the parents and therefore ensure the genetic diversity of the subsequent generation of children. As this process is iterated, a sequence of successive generations evolves and the average fitness of the chromosomes tends to increase until stopping criterion is reached. Two functions which needed to be optimized are defined as follows:</p><script type="math/tex; mode=display">    \begin{aligned}    Max \ f_1(x , y) &= \frac{sinx}{x}\frac{siny}{y} \\     &x,y \in [-10.0000,10.0000]    \end{aligned}</script><script type="math/tex; mode=display">    \begin{aligned}    Max \ f_2(x , y) &= xsin(10\pi x) + ysin(10\pi y) \\    &x,y \in [-1.0000,2.0000]    \end{aligned}</script><h2 id="Encoding"><a href="#Encoding" class="headerlink" title="Encoding"></a>Encoding</h2><p>Binary encoding is implemented to represent the chromosome, which is a string of bits, 0 or 1. The bit strings with 10 genes can provide a solution space consisting <script type="math/tex">2^{10}</script> individuals. Take function <script type="math/tex">f_1</script> as an example, the domain of function <script type="math/tex">f_1</script> is <script type="math/tex">[-10, 10]</script>，and the standard binary encoding can only represent integer, which cannot satisfy the demanded accuracy. Thus, a linear mapping, which map domain: <script type="math/tex">D \in [-10.0000, 10.0000]</script> to <script type="math/tex">D^* \in [0, 200,000]</script>, is introduced. Obviously, a binary string, which has 18 genes, provides <script type="math/tex">262,144</script> diverse individuals. In conclusion, the chromosome with 18 genes and the chromosome with 15 genes is adopted, encoding the domains of function<script type="math/tex">f_1,f_2</script> respectively.</p><h2 id="Selection"><a href="#Selection" class="headerlink" title="Selection"></a>Selection</h2><p>Roulette wheel selection is a genetic operator for selecting potentially useful solutions for recombination. In roulette wheel selection, the fitness function assigns a fitness to generated individuals. The values, which are achieved by dividing the fitness of a selection by the total fitness of all the selections, determine the possibility that those parents can produce offsprings. If <script type="math/tex">f_i</script> is the fitness of individual <script type="math/tex">i</script> in the population, its probability of being selected is</p><script type="math/tex; mode=display">    p_i = \frac{f_i}{\sum_{j=1}^N f_j}</script><p>where <script type="math/tex">N</script> is the number of individuals in the population. This process is introduced in Algorithm 1:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;algorithm&#125;</span><br><span class="line">\caption&#123;Roulette Wheel Selection&#125;</span><br><span class="line">\begin&#123;algorithmic&#125;[1]</span><br><span class="line">\FOR&#123;each $fitness \in [1,IndividualsCount]$&#125;</span><br><span class="line">\IF&#123;$fitness$ is Negative&#125; </span><br><span class="line">\STATE $fitness \Leftarrow 0$;</span><br><span class="line">\ENDIF</span><br><span class="line">\ENDFOR</span><br><span class="line">\STATE $IndividualProb \Leftarrow IndividualFit/PopulationFit$;</span><br><span class="line">\STATE Calculate the $CumulativeSum$ of the $IndividualProb$;</span><br><span class="line">\STATE Sort a random $List \in [0, 1]$;</span><br><span class="line">\STATE $i \Leftarrow 1$;</span><br><span class="line">\STATE $j \Leftarrow 1$;</span><br><span class="line">\WHILE &#123;each $i \in [1,IndividualsCount]$&#125;</span><br><span class="line">\IF&#123;$List \leq CumulativeSum$&#125;</span><br><span class="line">\STATE $i_&#123;th&#125; NewIndividual \Leftarrow j_&#123;th&#125; OldIndividual$;</span><br><span class="line">\STATE $i \Leftarrow i + 1$; </span><br><span class="line">\ELSE</span><br><span class="line">\STATE $j \Leftarrow j + 1$;</span><br><span class="line">\ENDIF</span><br><span class="line">\ENDWHILE </span><br><span class="line">\label&#123;code:ag1&#125;</span><br><span class="line">\end&#123;algorithmic&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure><h2 id="Crossover"><a href="#Crossover" class="headerlink" title="Crossover"></a>Crossover</h2><p>Crossover is used to combine the genetic information of two parents to generate new offspring. Bit strings are used to be recombined by crossover. Uniform crossover, which uses binary mask, makes the crossover more flexible. By choosing a random crossover mask with the same length of parent’s chromosome, genes are copied from one parent when there is <script type="math/tex">0</script> in the crossover mask, the other genes are copied from the other parent when there is <script type="math/tex">1</script> in the crossover mask. Thus, offsprings contain a mixture of genes from each parent. In this case, the probability of crossover is <script type="math/tex">0.6</script>.</p><h2 id="Mutation"><a href="#Mutation" class="headerlink" title="Mutation"></a>Mutation</h2><p>Mutation is used to maintain and introduce diversity in the genetic population. Bit flip mutation is implemented in our solutions, a certain number of bits was selected to perform flip. Mutation happens with a low frequency, probability of mutation occurrence is selected as 0.001 to 0.01. In this case, mutation occurred once on 2 random genes whose population has 288 genes per iteration. </p><h2 id="Algorithm"><a href="#Algorithm" class="headerlink" title="Algorithm"></a>Algorithm</h2><p>Genetic algorithm can be presented by the following algorithm.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;algorithm&#125;</span><br><span class="line">\caption&#123;Genetic Algorithm&#125;</span><br><span class="line">\begin&#123;algorithmic&#125;[1]</span><br><span class="line">\STATE SET individuals count $N$;</span><br><span class="line">\STATE SET genes count $M$;</span><br><span class="line">\STATE Generate a $N\times M$population matrix $\bm&#123;P&#125;$;</span><br><span class="line">\FOR&#123;each $iter \in [1,LargeNum]$&#125;</span><br><span class="line">\STATE Calculate fitness for each individual;</span><br><span class="line">\STATE \textbf&#123;call&#125; Algorithm 1;</span><br><span class="line">\IF&#123;$random(0 \to 1) &gt; SetProbability$&#125;</span><br><span class="line">\STATE Crossover with a random generated mask;</span><br><span class="line">\ENDIF</span><br><span class="line">\STATE $x \leftarrow random(0 \to N)$</span><br><span class="line">\STATE $y \leftarrow random(0 \to M)$</span><br><span class="line">\STATE Flip $\bm&#123;P&#125;_&#123;x,y&#125;$ </span><br><span class="line">\IF&#123;$timeout \cup isOptimalFound$&#125;</span><br><span class="line">\STATE \textbf&#123;break&#125;;</span><br><span class="line">\ENDIF</span><br><span class="line">\ENDFOR</span><br><span class="line">\label&#123;code:ag2&#125;</span><br><span class="line">\end&#123;algorithmic&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure></p><h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>The proposed algorithm has been implemented on analysis of <script type="math/tex">f_1</script> and <script type="math/tex">f_2</script>, the solution involves finding the maximal and its corresponding locations. The evolutionary processes are also presented in detail. </p><h2 id="Analysis-of-f-1"><a href="#Analysis-of-f-1" class="headerlink" title="Analysis of f_1"></a>Analysis of <script type="math/tex">f_1</script></h2><p>Apparently, function <script type="math/tex">f_1(x , y) = \frac{sinx}{x}\frac{siny}{y}</script>, whose domain is <script type="math/tex">x, y\in[-10,10]</script>, acquires the maximum value when <script type="math/tex">x, y \to 0</script>, nevertheless, <script type="math/tex">x, y</script> can not be zero. Under this circumstances, the fitness function used in genetic algorithm is defined by the following equation:</p><script type="math/tex; mode=display">f_1(x,y) = \left \{\begin{array}{l}\frac{sinx}{x}\frac{siny}{y} \ \ x, y\neq0 \\\frac{sinx}{x+\delta}\frac{siny}{y+\delta}\ \  x, y = 0 \end{array}\right.</script><p>where <script type="math/tex">\delta</script> is a small positive real number.</p><p>Because of the Roulette Wheel algorithm, which needs positive input, the values calculated by the fitness function may not satisfy that. Thus, we assume that those parents whose fitness is negative cannot produce offsprings. i.e, if the function <script type="math/tex">f_1 < 0</script>, let <script type="math/tex">f_1 = 0</script>. Algorithm parameter is listed in TABLE 1.</p><div class="table-container"><table><thead><tr><th style="text-align:left">Parameter Name</th><th style="text-align:right">Value</th></tr></thead><tbody><tr><td style="text-align:left">Iteration</td><td style="text-align:right">600</td></tr><tr><td style="text-align:left">Individuals Count</td><td style="text-align:right">8, 8 for x, y</td></tr><tr><td style="text-align:left">Genes Count</td><td style="text-align:right">18 per individual</td></tr><tr><td style="text-align:left">Probability of Crossover</td><td style="text-align:right">60%</td></tr><tr><td style="text-align:left">Probability of Mutation</td><td style="text-align:right">0.6%</td></tr></tbody></table></div><p>Genetic algorithm with 600 iteration finished within 0.133901 seconds on average, the highest fitness value achieved is 0.9998.  Those maximal fitness values are recorded on 100th, 200th, 300th iteration and the final iteration. The reason why data dropped between the 300th to final iteration is the fitness remain the same practically. Evolutionary process of 8 groups of individuals is shown in Fig. 1, the maximal fitness and its corresponding individuals are shown in TABLE Maximal fitness and groups, which are marked on the curve of <script type="math/tex">f_1(x, y)</script> in Fig. 2.</p><div class="table-container"><table><thead><tr><th style="text-align:right">N_th iteration</th><th style="text-align:right">Optimal Groups</th><th style="text-align:right">Optimal Fitness</th></tr></thead><tbody><tr><td style="text-align:right">100</td><td style="text-align:right">x=-0.3374,y=2.3562</td><td style="text-align:right">0.2944</td></tr><tr><td style="text-align:right">200</td><td style="text-align:right">x=-0.1838,y=0.6664</td><td style="text-align:right">0.9224</td></tr><tr><td style="text-align:right">300</td><td style="text-align:right">x=-0.1836,y=-0.1016</td><td style="text-align:right">0.9904</td></tr><tr><td style="text-align:right">600</td><td style="text-align:right">x=-0.0001,y=-0.0352</td><td style="text-align:right">0.9998</td></tr></tbody></table></div><div align="center">  <img width="300" src="/2019/01/24/genetic-algorithm/1.png"></div><p>In Fig. 1, the random generated 8 groups whose fitnesses are around <script type="math/tex">[0 , 0.25]</script> reached 0.9998(fitness) on average at the 230th iteration. We further analyze the varieties of each group, noticing that the improvement of the population’s average fitnesses depends on the mutations occur on the isolated group, which elevated the whole population by crossover. The process can be recognized in 70th, 110th and 260th iteration in  Fig. 1.</p><div align="center">  <img width="300" src="/2019/01/24/genetic-algorithm/2.png"></div><h2 id="Analysis-of-f-2"><a href="#Analysis-of-f-2" class="headerlink" title="Analysis of f_2"></a>Analysis of <script type="math/tex">f_2</script></h2><p>The function <script type="math/tex">f_2</script> is defined as follows:</p><script type="math/tex; mode=display">    f_2(x, y)= x sin(10\pi x) + y sin(10\pi y)  \ \ x, y \in [-1, 2]</script><p>where there is no need to tweak the function for satisfying the requirements of genetic algorithm except the domain. <script type="math/tex">x, y</script> in <script type="math/tex">f_2(x, y)</script> range from <script type="math/tex">[-1, 2]</script>, which are considered as the floating number with 4 decimal. That is, we can use chromosomes with 15 bits which have 32768 individuals to encode the domain. </p><p>Differ from function <script type="math/tex">f_1</script> which attained maximal value at $(0, 0)$, $f_2$’s maximal value at the “border” of the domain. Evidently, $f_2$ is symmetric with respect to the x-axis and the y-axis, so we investigate on the positive plane. To achieve maximum, for both x, y shall be the largest numbers in the domain which satisfy </p><script type="math/tex; mode=display">x, y = \frac{2k+\frac{1}{2}}{10} \ \ \ \ k \in N</script><p>Thus, those individuals generated randomly which range from <script type="math/tex">(30000, 32768]</script> must be rejected to ensure the correctness. In light of this, the following states shall be inserted between line 12-13 in Algorithm 2:<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;algorithm&#125;[!htbp]</span><br><span class="line">\caption&#123;Appendix States&#125;</span><br><span class="line">\begin&#123;algorithmic&#125;[1]</span><br><span class="line">\STATE Check the new individuals;</span><br><span class="line">\IF&#123;individuals out of range&#125;</span><br><span class="line">\STATE \textbf&#123;pass&#125;</span><br><span class="line">\ELSE</span><br><span class="line">\STATE $new\bm&#123;P&#125;_&#123;x, y&#125; \leftarrow \bm&#123;P&#125;_&#123;x, y&#125;$ </span><br><span class="line">\ENDIF</span><br><span class="line">\end&#123;algorithmic&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure></p><p>Algorithm parameter is listed in TABLE 3</p><div class="table-container"><table><thead><tr><th style="text-align:left">Parameter Name</th><th style="text-align:right">Value</th></tr></thead><tbody><tr><td style="text-align:left">Iteration</td><td style="text-align:right">1200</td></tr><tr><td style="text-align:left">Individuals Count</td><td style="text-align:right">8, 8 for x, y</td></tr><tr><td style="text-align:left">Genes Count</td><td style="text-align:right">15 per individual</td></tr><tr><td style="text-align:left">Probability of Crossover</td><td style="text-align:right">60%</td></tr><tr><td style="text-align:left">Probability of Mutation</td><td style="text-align:right">0.8%</td></tr></tbody></table></div><p>Genetic algorithm with 1200 iteration finished within 0.2818147 seconds on average, the highest fitness value achieved was 3.6996, which is shown on Fig. 3:  </p><div align="center">  <img width="300" src="/2019/01/24/genetic-algorithm/3.png"></div><p>Compared to the fix-step logging strategy for <script type="math/tex">f_1</script>, a variable-step strategy was applied to log the evolutionary progress in this section. If a gap with a certain threshold existed between the maximal fitness calculated by the current population and the last population, we call location who gains the first maximal fitness after the gap <script type="math/tex">node(M, i)</script> where M is gap threshold and i is the iteration after a gap. By connecting the nodes, we obtained the evolutionary progress, EP for short. A sample epoch result with 4 nodes is listed in TABLE 4:</p><div class="table-container"><table><thead><tr><th style="text-align:right">N_th iteration</th><th style="text-align:right">Optimal Groups</th><th style="text-align:right">Optimal Fitness</th></tr></thead><tbody><tr><td style="text-align:right">4</td><td style="text-align:right">x=1.6486,y=-0.4975</td><td style="text-align:right">1.6890</td></tr><tr><td style="text-align:right">15</td><td style="text-align:right">x=1.8262,y=1.2434</td><td style="text-align:right">2.5558</td></tr><tr><td style="text-align:right">41</td><td style="text-align:right">x=1.8514,y=1.4544</td><td style="text-align:right">3,2901</td></tr><tr><td style="text-align:right">157</td><td style="text-align:right">x=1.8512,y=1.8498</td><td style="text-align:right">3.6996</td></tr></tbody></table></div><div align="center">  <img width="300" src="/2019/01/24/genetic-algorithm/4.png"></div><p>Fig. 4 shows a sample progress by connecting <script type="math/tex">node(0.1, i)</script>, as the figure shows, evolutionary progress started at <script type="math/tex">node(0.1, 1)</script> doesn’t have plenty of chatter, hence this sample are considered as an efficient evolutionary progress.</p><h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>This report has presented the solutions to function optimization problem. In general, we have some interesting observations on genetic algorithms. The evolutionary progress demonstrated genetic algorithm may show an ultimate efficiency though lack robustness hence the stop criterion is not clear in every problem. Moreover, we noticed that genetic algorithms have a tendency to converge towards local optimum, an evolutionary progress may stuck at some certain points for hundreds of iterations, depending on random mutations. Another problem is we can’t simply deploy parallel computing to accelerate the evolutionary progress because the new population are generated by the last population which has a sequential logic. </p><h1 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h1><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br></pre></td><td class="code"><pre><span class="line">clear</span><br><span class="line">chroLen = <span class="number">15</span>; chroNum = <span class="number">16</span>; <span class="built_in">i</span> = <span class="number">1</span>;</span><br><span class="line">chroList = <span class="built_in">zeros</span>(chroNum, chroLen);</span><br><span class="line">list = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">1000</span>);</span><br><span class="line"><span class="comment">%% generate chromosome</span></span><br><span class="line"><span class="keyword">while</span>(<span class="built_in">i</span> &lt;= chroNum)</span><br><span class="line">    preChk = <span class="built_in">round</span>(<span class="built_in">rand</span>(<span class="number">1</span>,chroLen));</span><br><span class="line">    <span class="keyword">if</span> b2d(preChk) &lt; <span class="number">3e4</span></span><br><span class="line">        chroList(<span class="built_in">i</span>,:) = preChk;</span><br><span class="line">        <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> step = <span class="number">1</span>:<span class="number">600</span></span><br><span class="line">    <span class="comment">%% calculate fitness</span></span><br><span class="line">    a = chroList(<span class="number">1</span>:<span class="number">8</span>,:);b = chroList(<span class="number">9</span>:<span class="number">16</span>,:);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:chroNum/<span class="number">2</span></span><br><span class="line">        fitness(<span class="built_in">i</span>) = calcFitness(a(<span class="built_in">i</span>,:),b(<span class="built_in">i</span>,:));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment">%% select chromosome</span></span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:chroNum/<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> fitness(<span class="built_in">i</span>) &lt; <span class="number">0</span></span><br><span class="line">            fitness(<span class="built_in">i</span>) = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    fitness_ = <span class="number">0.0001</span>+fitness;</span><br><span class="line">    sum_fitness = sum(fitness_);</span><br><span class="line">    proba = cumsum(fitness_/sum_fitness);</span><br><span class="line">    newChroList = <span class="built_in">zeros</span>(chroNum,chroLen);</span><br><span class="line">    rndList = <span class="built_in">sort</span>(<span class="built_in">rand</span>(chroNum/<span class="number">2</span>,<span class="number">1</span>));</span><br><span class="line">    <span class="built_in">i</span> = <span class="number">1</span>; <span class="built_in">j</span> = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">while</span> <span class="built_in">i</span> &lt;= chroNum/<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> rndList(<span class="built_in">i</span>) &lt;= proba(<span class="built_in">j</span>)</span><br><span class="line">            newChroList(<span class="built_in">i</span>,:) = chroList(<span class="built_in">j</span>,:);</span><br><span class="line">            newChroList(<span class="built_in">i</span>+<span class="number">8</span>,:) = chroList(<span class="built_in">j</span>+<span class="number">8</span>,:);</span><br><span class="line">            <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="built_in">j</span> = <span class="built_in">j</span> + <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment">%% crossover</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">rand</span>(<span class="number">1</span>) &gt; <span class="number">0.6</span>)</span><br><span class="line">        swap = <span class="built_in">zeros</span>(<span class="number">1</span>,chroLen);</span><br><span class="line">        mask = logical(<span class="built_in">round</span>(<span class="built_in">rand</span>(<span class="number">1</span>,chroLen)));</span><br><span class="line">        <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">2</span>:<span class="number">15</span></span><br><span class="line">            swap(<span class="number">1</span>,mask) = newChroList(<span class="built_in">i</span>,mask);</span><br><span class="line">            newChroList(<span class="built_in">i</span>,mask) = ...</span><br><span class="line">            newChroList(<span class="built_in">i</span>+<span class="number">1</span>,mask);</span><br><span class="line">            newChroList(<span class="built_in">i</span>+<span class="number">1</span>,mask) = swap(<span class="number">1</span>,mask); </span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="comment">%% mutation</span></span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">rand</span>(<span class="number">1</span>) &gt; <span class="number">0</span>)</span><br><span class="line">        <span class="built_in">i</span> = <span class="built_in">floor</span>(<span class="built_in">numel</span>(newChroList)...</span><br><span class="line">        *<span class="built_in">rand</span>(<span class="number">1</span>)/chroLen)+<span class="number">1</span>;</span><br><span class="line">        m = <span class="built_in">floor</span>(<span class="built_in">numel</span>(newChroList)...</span><br><span class="line">        *<span class="built_in">rand</span>(<span class="number">1</span>)/chroLen)+<span class="number">1</span>;</span><br><span class="line">        <span class="built_in">j</span> = <span class="built_in">mod</span>(<span class="built_in">round</span>(<span class="built_in">numel</span>(newChroList)...</span><br><span class="line">        *<span class="built_in">rand</span>(<span class="number">1</span>)),chroLen)+<span class="number">1</span>;</span><br><span class="line">        n = <span class="built_in">mod</span>(<span class="built_in">round</span>(<span class="built_in">numel</span>(newChroList)...</span><br><span class="line">        *<span class="built_in">rand</span>(<span class="number">1</span>)),chroLen)+<span class="number">1</span>;</span><br><span class="line">        newChroList(<span class="built_in">i</span>,<span class="built_in">j</span>) = not(newChroList(<span class="built_in">i</span>,<span class="built_in">j</span>));</span><br><span class="line">        newChroList(m,n) = not(newChroList(m,n));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    a = newChroList(<span class="number">1</span>:<span class="number">8</span>,:); </span><br><span class="line">    b = newChroList(<span class="number">9</span>:<span class="number">16</span>,:);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:chroNum/<span class="number">2</span></span><br><span class="line">        newFitness(<span class="built_in">i</span>) = ...</span><br><span class="line">        calcFitness(a(<span class="built_in">i</span>,:),b(<span class="built_in">i</span>,:));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">max</span>(newFitness) &gt;= <span class="built_in">max</span>(fitness)</span><br><span class="line">        <span class="keyword">if</span> ~chk(newChroList)</span><br><span class="line">            chroList = newChroList;</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">calcFitness</span><span class="params">(a , b)</span>    </span></span><br><span class="line">    a_ = (b2d(a)<span class="number">-1e5</span>)/<span class="number">1e4</span>; b_ = (b2d(b)<span class="number">-1e5</span>)/<span class="number">1e4</span>;</span><br><span class="line">    y = <span class="built_in">sin</span>(a_)/a_*<span class="built_in">sin</span>(b_)/b_;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h1 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerlink&quot; title=&quot;Introduction&quot;&gt;&lt;/a&gt;Introduction&lt;/h1&gt;&lt;p&gt;Inspired by the process of natural selection and principle of genetics, genetic algorithms are implemented to handle various optimization problems. Basic theorem and  mathematical derivation has given by J. H. Holland. This research report is intended as a solution, performing optimization on two fitness functions. The reminder of this report is structured as follows. Section 2 reviews the relevant fundamentals of genetic algorithm, which includes the detailed structure with its component parts. In Section 3, the maximal of two fitness function are given with the optimal populations which contain exhaustive revolutionary process. Conclusions of the report are presented in Section 4.&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="Algorithm" scheme="https://rm-rf.moe/tags/Algorithm/"/>
    
  </entry>
  
</feed>
