<!DOCTYPE html><html lang="en"><head><meta name="generator" content="Hexo 3.8.0"><meta charset="utf-8"><meta name="X-UA-Compatible" content="IE=edge"><title> Neural Network-Assignment · 梦的小窝</title><meta name="description" content="Neural Network-Assignment - hsm"><meta name="viewport" content="width=device-width, initial-scale=1"><link rel="icon" href="/favicon.png"><link rel="stylesheet" href="/css/apollo.css"><link rel="search" type="application/opensearchdescription+xml" href="https://rm-rf.moe/atom.xml" title="梦的小窝"></head><body><div class="wrap"><header><a href="/" class="logo-link"><img src="/favicon.png" alt="logo"></a><ul class="nav nav-list"><li class="nav-list-item"><a href="/" target="_self" class="nav-list-link">BLOG</a></li><li class="nav-list-item"><a href="/archives/" target="_self" class="nav-list-link">ARCHIVE</a></li><li class="nav-list-item"><a href="/about/index.html" target="_self" class="nav-list-link">ABOUT</a></li><li class="nav-list-item"><a href="/atom.xml" target="_self" class="nav-list-link">RSS</a></li></ul></header><main class="container"><div class="post"><article class="post-block"><h1 class="post-title">Neural Network-Assignment</h1><div class="post-info">Feb 1, 2019</div><div class="post-content"><h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p>Artificial neural networks (ANNs) are computational models that are loosely inspired by their biological counterparts. Artificial neurons, which are elementary units in an ANN, appears to have been first introduced by McCulloch and Pitts in 1943 to demonstrate the biological neurons using algorithms and mathematics. An ANN is based on a set of connected artificial neurons. Each connection, like the synapses in a biological brain, can transmit a signal from one artificial neuron to another. Since the early work of Hebb, who proposed cell assembly theory which attempted to explain synaptic plasticity, considerable effort has been devoted to ANNs. Though perceptron was created by Rosenblatt for pattern recognition by training an adaptive  McCulloch-Pitts neuron model, it is incapable of processing the exclusive-or circuit. A key trigger for renewed interest in neural networks and learning was Werbos’s back-propagation algorithm, which makes the training of multi-layer networks feasible and efficient.<br><a id="more"></a></p>
<h1 id="Methods"><a href="#Methods" class="headerlink" title="Methods"></a>Methods</h1><p>Multi-Layered Perceptron(MLP) gains prestige as a feedforward neural network, which has been the most widely used neural networks. Error Back Propagation(BP) algorithm is implemented as a training method for MLPs.</p>
<h2 id="Multi-Layered-Perceptron"><a href="#Multi-Layered-Perceptron" class="headerlink" title="Multi-Layered Perceptron"></a>Multi-Layered Perceptron</h2><p>MLP, which is a fully-connected feedforward neural network, uses a supervised learning methods. An MLP consists of an input layer, an output layer and at least one hidden layer. The fundamental structure of MLP is shown schematically in Fig. 1:</p>
<div align="center">
  <img width="300" src="/2019/02/01/neural-network/1.png">
</div>

<p>Considering the input layer, neurons take effect on passing the information, i.e. the activation functions of this layer are identity functions. Each neurons, who consist the input layer, completely connect with the first hidden layer, thus those neurons hold multiple outputs. By giving <script type="math/tex">M</script> neurons in input layer and <script type="math/tex">N</script> neurons in hidden layer, each connection is assigned a weight <script type="math/tex">v_{nm}</script>. A bias term added to total weighted sum of inputs to serve as threshold to shift the activation function.The propagation function computes the hidden layer input <script type="math/tex">X_j</script> to the neuron <script type="math/tex">x(j)</script> from the outputs of predecessor neurons <script type="math/tex">Z_j</script> has the form</p>
<script type="math/tex; mode=display">
X_j = \sum_i z_j v_{ij} - \theta</script><p>where <script type="math/tex">\theta</script> donates the bias term.</p>
<p>The activation function, which defines the output of the node, were chosen as “Hyperbolic tangent” and “Softsign” in this case. Equations, plot and derivative are given by TABLE 1.</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/2.png">
</div>

<p>MLPs’ scheme essentially has three features:</p>
<ul>
<li>There are no connections between the neurons which are in the layers, and neurons shall not have connections with themselves.</li>
<li>Full connections exist only between the nearby layers.</li>
<li>It consists of two aspects: the feed-forward transmission of information and the feed-back transmission of error.</li>
</ul>
<h2 id="Error-Back-Propagation"><a href="#Error-Back-Propagation" class="headerlink" title="Error Back Propagation"></a>Error Back Propagation</h2><p>Back Propagation is a method used in ANNs to calculate a gradient that is needed in the calculation of the weights between layers, and it is commonly used by the gradient descent optimization algorithm. Calculating adjustments of weights in Fig. 1 is chosen as an example to introduce BP algorithm. We represent the parameter <script type="math/tex">\delta</script> with respect to <script type="math/tex">i</script>-th layer by the notation <script type="math/tex">\delta^{(i)}</script>. </p>
<p>For calculating the weights between hidden layer and output layer, loss function is defined as:</p>
<script type="math/tex; mode=display">
E_q = \frac{1}{2} \sum^{K}_{j=1} (\widehat{y}_{qj}-y_{qj})^2</script><p>where <script type="math/tex">\widehat{y}_{qj}</script> is expected output, <script type="math/tex">y_{qj}</script> is actual output. Based on gradient descent algorithm with learning rate <script type="math/tex">\mu</script>, which controls how much algorithm adjusting the weights of neuron network with respect the loss gradient, and the input of activation function <script type="math/tex">s_j</script>, we have</p>
<script type="math/tex; mode=display">
\Delta w_{ji} = -\mu \frac{\partial E_q}{\partial w_{ji}} = -\mu \frac{\partial E_q}{\partial s^{(3)}_j}\frac{\partial s^{(3)}_j}{\partial w_{ji}}</script><p>with</p>
<script type="math/tex; mode=display">
\frac{\partial s^{(3)}_j}{\partial w_{ji}} = \frac{\partial}{\partial w_{ji}}(\sum^N_{h=1} w_{jh}x_h) = x_i</script><p>and we have</p>
<script type="math/tex; mode=display">
\frac{\partial E_q}{\partial s^{(3)}_j} = -[\widehat{y}_{qj}-f(s^{(3)}_j)]f'(s^{(3)}_j) \triangleq \delta^{(3)}_j</script><p>where <script type="math/tex">\delta_j</script> donates the error of the output layer, thus, the adjustment of $w$ shall be:</p>
<script type="math/tex; mode=display">
\Delta w_{ji} = \mu \delta_j x_i</script><p>Consider calculating the weights between input layer and output layer:</p>
<script type="math/tex; mode=display">
\Delta v_{ji} = -\mu \frac{\partial E_q}{\partial v_{ji}} = -\mu \frac{\partial E_q}{\partial s^{(2)}_j}\frac{\partial s^{(2)}_j}{\partial v_{ji}}</script><p>with</p>
<script type="math/tex; mode=display">
\frac{\partial s^{(2)}_j}{\partial v_{ji}} = \frac{\partial}{\partial v_{ji}}(\sum^M_{h=1} v_{jh}z_h) = z_i</script><p>and</p>
<script type="math/tex; mode=display">
\frac{\partial E_q}{\partial s^{(2)}_j} = \frac{\partial E_q}{\partial x_j}\frac{\partial x_j}{\partial s^{(2)}_j}</script><p>where</p>
<script type="math/tex; mode=display">
\frac{\partial E_q}{\partial x_j}  = -\sum^K_{k = 1}(\widehat{y}_{qk} - y_k)f'(s_k^{(3)})w_{kj}\triangleq -\delta_j^{(2)}</script><p>thus</p>
<script type="math/tex; mode=display">
v_{ji} = -\mu \frac{\partial E_q}{\partial v_{ji}} = \mu \delta^{(2)}_j z_j</script><p>In summary,  the equations describing the adjustment of weights is:</p>
<script type="math/tex; mode=display">
w^{(l)}_{ji}[k+1] = w^{(l)}_{ji}[k] + \mu^{(l)}\delta_j^{(l)}x_i^{(l-1)}</script><p>where <script type="math/tex">\delta^{(l)}_j</script> for output layer is:</p>
<script type="math/tex; mode=display">
\delta^{(l)}_j = (\widehat{y} - y^{(l)})f'(s^{(l)}_j)</script><p>for hidden layer and input layer is</p>
<script type="math/tex; mode=display">
\delta^{(l)}_j = f'(s^{(l)}_j)\sum_{k = 1}^{n_{l+1}}\delta^{(l+1)}_k w^{(l+1)}_{kj}</script><p>respectively.</p>
<p>The process of training MLP with BP algorithm is shown in Algorithm 1.<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">\begin&#123;algorithm&#125;</span><br><span class="line">\caption&#123;Error Back Propagation Algorithm&#125;</span><br><span class="line">\begin&#123;algorithmic&#125;[1]</span><br><span class="line">\STATE SET initial weights $w^&#123;l&#125;_&#123;ji&#125;$, bias $\theta^&#123;l&#125;_&#123;j&#125;$;</span><br><span class="line">\STATE SET training sample $(x_q, y_q)$;</span><br><span class="line">\FOR&#123;each $iter \in [1, maxEpoch]$&#125;</span><br><span class="line">\STATE $x^&#123;(l)&#125;  = f(s^&#123;(l)&#125;) = f(W^&#123;(l)&#125;x^&#123;(l-1)&#125;)$</span><br><span class="line">\STATE $\delta^&#123;(l)&#125;_j = (\widehat&#123;y&#125; - y^&#123;(l)&#125;)f&apos;(s^&#123;(l)&#125;_j)$ for output layer</span><br><span class="line">\STATE $\delta^&#123;(l)&#125;_j = f&apos;(s^&#123;(l)&#125;_j)\sum_&#123;k = 1&#125;^&#123;n_&#123;l+1&#125;&#125;\delta^&#123;(l+1)&#125;_k w^&#123;(l+1)&#125;_&#123;kj&#125;$ for other layers</span><br><span class="line">\STATE $w^&#123;(l)&#125;_&#123;ji&#125;[k + 1] = w^&#123;(l)&#125;_&#123;ji&#125;[k] + \mu \delta_j^&#123;(l)&#125;x_i^&#123;(l-1)&#125;$</span><br><span class="line">\STATE $\theta^&#123;(l)&#125;_j[k + 1] = \theta^&#123;(l)&#125;_j[k] + \mu \delta_j^&#123;(l)&#125;$ </span><br><span class="line">\STATE Calculate $sumLoss = sumLoss + E_q$</span><br><span class="line">\IF&#123;$sumLoss/iter &lt; setLossTolerant$&#125;</span><br><span class="line">\STATE \textbf&#123;break&#125;;</span><br><span class="line">\ENDIF</span><br><span class="line">\ENDFOR</span><br><span class="line">\label&#123;code:ag1&#125;</span><br><span class="line">\end&#123;algorithmic&#125;</span><br><span class="line">\end&#123;algorithm&#125;</span><br></pre></td></tr></table></figure></p>
<h1 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h1><p>In this section, we have designed different model structures for addressing curve fitting problems.</p>
<h2 id="Network-Design"><a href="#Network-Design" class="headerlink" title="Network Design"></a>Network Design</h2><p>We designed two different structures, which are shown in Fig. 2.</p>
<div align="center">
  <img width="300" src="/2019/02/01/neural-network/3.png">
</div>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/4.png">
</div>

<p>The structure which has one hidden layer with 600 nodes was applied to fitting <script type="math/tex">f(x) = sin(x)</script> and <script type="math/tex">f(x_1, x_2) = \frac{sin(x_1)}{x_1} \frac{sin(x_2)}{x_2}</script>. For <script type="math/tex">y = |sin(x)|</script>, we chosen the structure with 2 hidden layers to reach a better regression performance. </p>
<h2 id="f-x-sin-x"><a href="#f-x-sin-x" class="headerlink" title="f(x) = sin(x)"></a><script type="math/tex">f(x) = sin(x)</script></h2><p>We trained the layers with 9 sample, using <script type="math/tex">E = \frac{1}{2} (\widehat{y} - y)^2</script> as the loss function. The training process will stop when <script type="math/tex">E_{avg} = (\sum_{i = 1}^{epoch} E_i) /epoch < 2\times 10^{-4}</script>. The results with 361 test samples are shown in Fig. 3:</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/5.png">
</div>

<p>Runtime loss $E$ and average loss <script type="math/tex">E_{avg}</script> are shown in Fig. 4, where the testing value evaluated by the average loss function is <script type="math/tex">9.3599\times 10^{-5}</script>.</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/6.png">
</div>

<p>We notice that the loss suffers a drastic drop within <script type="math/tex">50,000</script> epochs, then, to decrease the average loss <script type="math/tex">E_{avg}</script> from <script type="math/tex">0.1</script> to <script type="math/tex">0.002</script>, more than <script type="math/tex">1\times 10^6</script> epochs are needed. </p>
<p>The absolute error is shown in Fig. 5:</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/7.png">
</div>

<p>Apparently, max error between actual value and predict value is less than 0.05, which shows the prestige performance when fitting <script type="math/tex">f(x) = sin(x)</script>.</p>
<h2 id="f-x-sin-x-1"><a href="#f-x-sin-x-1" class="headerlink" title="f(x) = |sin(x)|"></a><script type="math/tex">f(x) = |sin(x)|</script></h2><p>When it comes to a more complex nonlinear curve, the fitting error using elementary MLP structure which only has one hidden layer is far from satisfactory. A hidden layer with softsign activation function was added to the basic MLP structure. By implementing the structure with 9 training sample, we arrive at the following results:</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/8.png">
</div>

<p>Fig. 7 shows the loss decreased over training epochs:</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/9.png">
</div>

<p>From the figure above, we can conclude that though the loss suffers the chattering within 500,000 epochs, while the average loss obtains a smooth constant rate of descent. Though the training loss which is less than <script type="math/tex">1\times 10^{-5}</script> after <script type="math/tex">3\times 10^6</script> epochs yields the guaranteed performance on training samples, as we can observed in Fig. 6, the test error(more than 0.2) is unbearable for some test samples. We assumed lacking of train samples has brought about the overfitting of neural network, thus we expanded the training sample with 29 independent (x, y) to reach the better performance. </p>
<p>When average loss is less than 0.005, we obtained the following result, as shown in Fig. 8:</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/10.png">
</div>

<p>Apparently, <script type="math/tex">y = |sin(x)|</script> has a non-differentiable point in <script type="math/tex">(0, 2\pi)</script>. From Fig. 9, the point which is non-differentiable has the maximal error compared to other points. i.e. providing more training sample can not reinforce the ability of non-linear fitting for a certain MLP model. </p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/11.png">
</div>

<p>Still, adding training sample improves the performance of prediction on test sets. From Fig. 9, though we have a test point which posses the error more than 0.1, absolute errors of other test points are less than 0.02. i.e. this approach provides more robust solutions than 9 training sample with the same loss tolerant. The training loss of this approach is shown in Fig. 10</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/12.png">
</div>

<h2 id="f-x-y-frac-sin-x-x-frac-sin-y-y"><a href="#f-x-y-frac-sin-x-x-frac-sin-y-y" class="headerlink" title="f(x, y) = \frac{sin(x)}{x}\frac{sin(y)}{y}"></a><script type="math/tex">f(x, y) = \frac{sin(x)}{x}\frac{sin(y)}{y}</script></h2><p>In this section, let us consider a function which has two inputs. <script type="math/tex">11\times 11</script> samples are given for training with the basic MLP structure. The hidden layer has 500 nodes, and the training will stop when <script type="math/tex">E_{avg} < 0.002</script>. Fig. 11 shows the actual curve and the curve predicted with <script type="math/tex">21\times 21</script> samples respectively.</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/13.png">
</div>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/14.png">
</div>

<p>We plot <script type="math/tex">E</script> and <script type="math/tex">E_{avg}</script>, varying <script type="math/tex">epoch</script> from <script type="math/tex">[1, 100,000]</script> in Fig. 12, from which we observe <script type="math/tex">E_{avg}</script> decrease rapidly from 1.1 to 0.1 within 1000 epochs. Thus, the training process shows the efficiency and robust performance of back propagation algorithm.</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/15.png">
</div>

<p>The test error is shown as Fig. 13, from which we observe the maximal training error is less than 0.15.</p>
<div align="center">
  <img width="350" src="/2019/02/01/neural-network/16.png">
</div>

<h1 id="Conclusions"><a href="#Conclusions" class="headerlink" title="Conclusions"></a>Conclusions</h1><p>In this report, we implemented multilayer perceptron to provide the solutions to three specific non-linear functions. Simulation results illustrates the accuracy of artificial neural networks and the efficiency of back propagation algorithm. </p>
<h1 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h1><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%% For y = sin(x)</span></span><br><span class="line">clear</span><br><span class="line">sample_num = <span class="number">9</span>; N1 = <span class="number">600</span>; Lr = <span class="number">0.005</span>;</span><br><span class="line">train_x = <span class="built_in">zeros</span>(<span class="number">1</span>,sample_num);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:sample_num</span><br><span class="line">    train_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(sample_num<span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">train_y = <span class="built_in">sin</span>(train_x);</span><br><span class="line"></span><br><span class="line">w1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">w2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">sum_e = <span class="number">0</span>; er = []; <span class="built_in">i</span> = <span class="number">0</span>; ei = [];</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    <span class="built_in">j</span> = unidrnd(sample_num);</span><br><span class="line">    in = train_x(<span class="built_in">j</span>);</span><br><span class="line">    L1 = w1*in+b1;</span><br><span class="line">    L2 = <span class="built_in">tanh</span>(L1);</span><br><span class="line">    L3 = sum(w2.*L2)+b2;</span><br><span class="line">    e = train_y(<span class="built_in">j</span>)-L3;</span><br><span class="line">    sum_e = sum_e + <span class="number">0.5</span>*e^<span class="number">2</span>;</span><br><span class="line">    er = [er sum_e/<span class="built_in">i</span>];</span><br><span class="line">    ei = [ei e];</span><br><span class="line">    sum_e/<span class="built_in">i</span></span><br><span class="line">    <span class="keyword">if</span> sum_e/<span class="built_in">i</span> &lt; <span class="number">0.002</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    delta_2 = e;</span><br><span class="line">    delta_1 = (<span class="number">1</span>-<span class="built_in">tanh</span>(L1).^<span class="number">2</span>).*(delta_2*w2);</span><br><span class="line">    w2 = w2+Lr*delta_2.*L2;</span><br><span class="line">    w1 = w1+Lr*delta_1*in;</span><br><span class="line">    b1 = b1+Lr*delta_1;</span><br><span class="line">    b2 = b2+Lr*e;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">test_x = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">1000</span>);</span><br><span class="line">test_y = [];</span><br><span class="line"><span class="built_in">true</span>_y = [];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">1000</span></span><br><span class="line">    test_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(<span class="number">1000</span><span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = test_x</span><br><span class="line">    test_y = [test_y sum(w2.*<span class="built_in">tanh</span>(w1*<span class="built_in">i</span>+b1))+b2];</span><br><span class="line">    <span class="built_in">true</span>_y = [<span class="built_in">true</span>_y <span class="built_in">sin</span>(<span class="built_in">i</span>)];</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">plot</span>(test_x, test_y, test_x, <span class="built_in">true</span>_y);</span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="comment">%% For y = abs(sin(x))</span></span><br><span class="line">clear</span><br><span class="line">sample_num = <span class="number">29</span>; N1 = <span class="number">500</span>; Lr = <span class="number">0.002</span>; </span><br><span class="line">train_x = <span class="built_in">zeros</span>(<span class="number">1</span>,sample_num);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:sample_num</span><br><span class="line">    train_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(sample_num<span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">train_y = <span class="built_in">abs</span>(<span class="built_in">sin</span>(train_x));</span><br><span class="line"></span><br><span class="line">w1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b3 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">w2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(N1,N1);</span><br><span class="line">w3 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">L3 = <span class="built_in">zeros</span>(<span class="number">1</span>,N1);</span><br><span class="line">delta_1 = <span class="built_in">zeros</span>(<span class="number">1</span>,N1);</span><br><span class="line">sum_e = <span class="number">0</span>; er = []; ei = []; <span class="built_in">i</span> = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">i</span> = <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    s = unidrnd(sample_num);</span><br><span class="line">    in = train_x(s);</span><br><span class="line">    L1 = w1*in+b1;</span><br><span class="line">    L2 = softsign(L1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:N1 </span><br><span class="line">        L3(<span class="built_in">j</span>) = sum(w2(<span class="built_in">j</span>,:).*L2)+b2(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    L4 = <span class="built_in">tanh</span>(L3);</span><br><span class="line">    L5 = sum(w3.*L4)+b3;</span><br><span class="line">    e = train_y(s)-L5;</span><br><span class="line">    sum_e = sum_e + <span class="number">0.5</span>*e^<span class="number">2</span>;</span><br><span class="line">    sum_e/<span class="built_in">i</span></span><br><span class="line">    <span class="keyword">if</span> sum_e/<span class="built_in">i</span> &lt; <span class="number">0.005</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    er = [er sum_e/<span class="built_in">i</span>];</span><br><span class="line">    ei = [ei e];</span><br><span class="line">    delta_3 = e;</span><br><span class="line">    delta_2 = (<span class="number">1</span>-<span class="built_in">tanh</span>(L2).^<span class="number">2</span>).*(delta_3*w3);</span><br><span class="line">    temp = dsoftsign(L1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:N1</span><br><span class="line">        delta_1(<span class="built_in">j</span>) = temp(<span class="built_in">j</span>).*...</span><br><span class="line">        sum(delta_2.*w2(<span class="built_in">j</span>,:));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    w3 = w3+Lr*delta_3.*L4;</span><br><span class="line">    w2 = w2+Lr*delta_2.*L2;</span><br><span class="line">    w1 = w1+Lr*delta_1*in;</span><br><span class="line">    b1 = b1+Lr*delta_1;</span><br><span class="line">    b2 = b2+Lr*delta_2;</span><br><span class="line">    b3 = b3+Lr*delta_3;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">test_x = <span class="built_in">zeros</span>(<span class="number">1</span>,<span class="number">1000</span>); t = <span class="built_in">zeros</span>(<span class="number">1</span>,N1);</span><br><span class="line">test_y = []; <span class="built_in">true</span>_y = [];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="number">1000</span></span><br><span class="line">    test_x(<span class="built_in">i</span>) = (<span class="built_in">i</span><span class="number">-1</span>)*<span class="number">2</span>*<span class="built_in">pi</span>/(<span class="number">1000</span><span class="number">-1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = test_x</span><br><span class="line">    var = softsign(w1*<span class="built_in">i</span>+b1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:N1</span><br><span class="line">        t(<span class="built_in">j</span>) = sum(w2(<span class="built_in">j</span>,:).*var)+b2(<span class="built_in">j</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    test_y = [test_y sum(w3.*<span class="built_in">tanh</span>(t))+b3];</span><br><span class="line">    <span class="built_in">true</span>_y = [<span class="built_in">true</span>_y <span class="built_in">abs</span>(<span class="built_in">sin</span>(<span class="built_in">i</span>))];</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">plot</span>(test_x, test_y, test_x, <span class="built_in">true</span>_y);</span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="comment">%% For z = sin(x)/x*sin(y)/y</span></span><br><span class="line">clear</span><br><span class="line">sample_num = <span class="number">20</span>; N1 = <span class="number">500</span>; Lr = <span class="number">0.006</span>;</span><br><span class="line">x1 = <span class="built_in">linspace</span>(<span class="number">-10</span>,<span class="number">10</span>,sample_num);</span><br><span class="line">x2 = <span class="built_in">linspace</span>(<span class="number">-10</span>,<span class="number">10</span>,sample_num);</span><br><span class="line">y = <span class="built_in">zeros</span>(sample_num,sample_num);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:sample_num</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:sample_num</span><br><span class="line">        y(<span class="built_in">i</span>,<span class="built_in">j</span>) = value(x1(<span class="built_in">i</span>),x2(<span class="built_in">j</span>));</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">w1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">2</span>,N1);</span><br><span class="line">b1 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">b2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,<span class="number">1</span>);</span><br><span class="line">w2 = <span class="number">0.1</span>*<span class="built_in">randn</span>(<span class="number">1</span>,N1);</span><br><span class="line">er = []; sum_e = <span class="number">0</span>; <span class="built_in">i</span> = <span class="number">0</span>; ei = [];</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span>(<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">i</span> =  <span class="built_in">i</span> + <span class="number">1</span>;</span><br><span class="line">    batch1 = unidrnd(sample_num);</span><br><span class="line">    batch2 = unidrnd(sample_num);</span><br><span class="line">    in1 = x1(batch1);</span><br><span class="line">    in2 = x2(batch2);</span><br><span class="line">    L1 = w1(<span class="number">1</span>,:)*in1+w1(<span class="number">2</span>,:)*in2+b1;</span><br><span class="line">    L2 = <span class="built_in">tanh</span>(L1);</span><br><span class="line">    L3 = sum(w2.*L2)+b2;</span><br><span class="line">    e = y(batch1, batch2)-L3;</span><br><span class="line">    sum_e = sum_e + <span class="number">0.5</span>*e^<span class="number">2</span>;</span><br><span class="line">    sum_e/<span class="built_in">i</span></span><br><span class="line">    er = [er sum_e/<span class="built_in">i</span>];</span><br><span class="line">    ei = [ei e];</span><br><span class="line">    <span class="keyword">if</span> sum_e/<span class="built_in">i</span> &lt; <span class="number">0.002</span></span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    delta_2 = e;</span><br><span class="line">    delta_1 = (<span class="number">1</span>-<span class="built_in">tanh</span>(L1).^<span class="number">2</span>).*(delta_2*w2);</span><br><span class="line">    w2 = w2+Lr*delta_2.*L2;</span><br><span class="line">    w1(<span class="number">1</span>,:) = w1(<span class="number">1</span>,:)+Lr*delta_1*in1;</span><br><span class="line">    w1(<span class="number">2</span>,:) = w1(<span class="number">2</span>,:)+Lr*delta_1*in2;</span><br><span class="line">    b1 = b1+Lr*delta_1;</span><br><span class="line">    b2 = b2+Lr*e;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">[X,Y] = <span class="built_in">meshgrid</span>(<span class="number">-10</span>:<span class="number">0.3</span>:<span class="number">10</span>);</span><br><span class="line">Z_ = (<span class="built_in">sin</span>(X)./X).*(<span class="built_in">sin</span>(Y)./Y);</span><br><span class="line">Z = <span class="built_in">zeros</span>(<span class="built_in">length</span>(X));</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:<span class="built_in">length</span>(X)</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span>:<span class="built_in">length</span>(X)</span><br><span class="line">        temp = <span class="built_in">tanh</span>(w1(<span class="number">1</span>, :)*X(<span class="built_in">i</span>, <span class="built_in">j</span>)...</span><br><span class="line">        +w1(<span class="number">2</span>,:)*Y(<span class="built_in">i</span>, <span class="built_in">j</span>)+b1);</span><br><span class="line">        Z(<span class="built_in">i</span>, <span class="built_in">j</span>) = sum(w2.*temp)+b2;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">mesh(X,Y,Z)</span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">mesh(X,Y,Z_)</span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">softsign</span><span class="params">(x)</span></span></span><br><span class="line">	y = x./(<span class="number">1</span>+<span class="built_in">abs</span>(x));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">\<span class="keyword">end</span>&#123;lstlisting&#125;</span><br><span class="line">\begin&#123;lstlisting&#125;</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">y</span> = <span class="title">dsoftsign</span><span class="params">(x)</span></span></span><br><span class="line">	y = (<span class="number">1.</span>/(<span class="number">1</span>+x).^<span class="number">2</span>).*(x&gt;=<span class="number">0</span>)+(<span class="number">1.</span>/(<span class="number">1</span>-x).^<span class="number">2</span>).*(x&lt;<span class="number">0</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
</div></article></div></main><footer><div class="paginator"><a href="/2019/01/24/genetic-algorithm/" class="next">NEXT</a></div><div class="copyright"><p>© 2019 <a href="https://rm-rf.moe">hsm</a>, powered by <a href="https://hexo.io/" target="_blank">Hexo</a> and <a href="https://github.com/pinggod/hexo-theme-apollo" target="_blank">hexo-theme-apollo</a>.</p></div></footer></div><script async src="//cdn.bootcss.com/mathjax/2.7.0/MathJax.js?config=TeX-MML-AM_CHTML" integrity="sha384-crwIf/BuaWM9rM65iM+dWFldgQ1Un8jWZMuh3puxb8TOY9+linwLoI7ZHZT+aekW" crossorigin="anonymous"></script></body></html>